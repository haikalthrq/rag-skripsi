  ================================================================================
                          QWEN3-EMBEDDING-4B-GGUF
                              DOCUMENTATION
  ================================================================================

  ================================================================================
  1. HIGHLIGHTS
  ================================================================================

  The Qwen3 Embedding model series is the latest proprietary model of the Qwen 
  family, specifically designed for text embedding and ranking tasks. Building 
  upon the dense foundational models of the Qwen3 series, it provides a 
  comprehensive range of text embeddings and reranking models in various sizes 
  (0.6B, 4B, and 8B). 

  This series inherits the exceptional multilingual capabilities, long-text 
  understanding, and reasoning skills of its foundational model. The Qwen3 
  Embedding series represents significant advancements in multiple text embedding 
  and ranking tasks, including:
    â€¢ Text retrieval
    â€¢ Code retrieval
    â€¢ Text classification
    â€¢ Text clustering
    â€¢ Bitext mining

  --------------------------------------------------------------------------------
  1.1 Exceptional Versatility
  --------------------------------------------------------------------------------
  The embedding model has achieved state-of-the-art performance across a wide 
  range of downstream application evaluations. The 8B size embedding model ranks 
  No.1 in the MTEB multilingual leaderboard (as of June 5, 2025, score 70.58), 
  while the reranking model excels in various text retrieval scenarios.

  --------------------------------------------------------------------------------
  1.2 Comprehensive Flexibility
  --------------------------------------------------------------------------------
  The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) 
  for both embedding and reranking models, catering to diverse use cases that 
  prioritize efficiency and effectiveness. 

  Features:
    â€¢ Developers can seamlessly combine embedding and reranking modules
    â€¢ Embedding model allows flexible vector definitions across all dimensions
    â€¢ Both embedding and reranking models support user-defined instructions to 
      enhance performance for specific tasks, languages, or scenarios

  --------------------------------------------------------------------------------
  1.3 Multilingual Capability
  --------------------------------------------------------------------------------
  The Qwen3 Embedding series offer support for over 100 languages, thanks to the 
  multilingual capabilities of Qwen3 models. This includes:
    â€¢ Various programming languages
    â€¢ Robust multilingual retrieval capabilities
    â€¢ Cross-lingual retrieval capabilities
    â€¢ Code retrieval capabilities


  ================================================================================
  2. MODEL OVERVIEW
  ================================================================================

  Qwen3-Embedding-4B-GGUF has the following features:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Feature                 â”‚ Specification                                        â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Model Type              â”‚ Text Embedding                                       â”‚
  â”‚ Supported Languages     â”‚ 100+ Languages                                       â”‚
  â”‚ Number of Parameters    â”‚ 4B                                                   â”‚
  â”‚ Context Length          â”‚ 32K                                                  â”‚
  â”‚ Embedding Dimension     â”‚ Up to 2560 (supports user-defined: 32 to 2560)       â”‚
  â”‚ Quantization Options    â”‚ q4_K_M, q5_0, q5_K_M, q6_K, q8_0, f16                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  For more details, including benchmark evaluation, hardware requirements, and 
  inference performance, please refer to our blog, GitHub.


  ================================================================================
  3. QWEN3 EMBEDDING SERIES MODEL LIST
  ================================================================================

  --------------------------------------------------------------------------------
  3.1 Text Embedding Models
  --------------------------------------------------------------------------------
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Model                  â”‚ Size â”‚ Layers â”‚ Seq Len  â”‚ Embed Dim â”‚ MRL     â”‚ Instruction â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Qwen3-Embedding-0.6B   â”‚ 0.6B â”‚ 28     â”‚ 32K      â”‚ 1024      â”‚ Yes     â”‚ Yes         â”‚
  â”‚ Qwen3-Embedding-4B     â”‚ 4B   â”‚ 36     â”‚ 32K      â”‚ 2560      â”‚ Yes     â”‚ Yes         â”‚
  â”‚ Qwen3-Embedding-8B     â”‚ 8B   â”‚ 36     â”‚ 32K      â”‚ 4096      â”‚ Yes     â”‚ Yes         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  --------------------------------------------------------------------------------
  3.2 Text Reranking Models
  --------------------------------------------------------------------------------
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Model                  â”‚ Size â”‚ Layers â”‚ Seq Len  â”‚ Embed Dim â”‚ MRL     â”‚ Instruction â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Qwen3-Reranker-0.6B    â”‚ 0.6B â”‚ 28     â”‚ 32K      â”‚ -         â”‚ -       â”‚ Yes         â”‚
  â”‚ Qwen3-Reranker-4B      â”‚ 4B   â”‚ 36     â”‚ 32K      â”‚ -         â”‚ -       â”‚ Yes         â”‚
  â”‚ Qwen3-Reranker-8B      â”‚ 8B   â”‚ 36     â”‚ 32K      â”‚ -         â”‚ -       â”‚ Yes         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  --------------------------------------------------------------------------------
  3.3 Notes
  --------------------------------------------------------------------------------
  â€¢ MRL Support: 
    Indicates whether the embedding model supports custom dimensions for the 
    final embedding.

  â€¢ Instruction Aware: 
    Notes whether the embedding or reranking model supports customizing the 
    input instruction according to different tasks.

  â€¢ Performance Improvement with Instructions:
    Our evaluation indicates that, for most downstream tasks, using instructions 
    (instruct) typically yields an improvement of 1% to 5% compared to not using 
    them. Therefore, we recommend that developers create tailored instructions 
    specific to their tasks and scenarios.

  â€¢ Language Recommendation for Instructions:
    In multilingual contexts, we advise users to write their instructions in 
    English, as most instructions utilized during the model training process 
    were originally written in English.


  ================================================================================
  4. USAGE
  ================================================================================

  ğŸ“Œ TIP: We recommend that developers customize the instruct according to their 
        specific scenarios, tasks, and languages. Our tests have shown that in 
        most retrieval scenarios, not using an instruct on the query side can 
        lead to a drop in retrieval performance by approximately 1% to 5%.

  --------------------------------------------------------------------------------
  4.1 llama.cpp
  --------------------------------------------------------------------------------
  Check out our llama.cpp documentation for more usage guide.

  We advise you to clone llama.cpp and install it following the official guide. 
  We follow the latest version of llama.cpp. In the following demonstration, we 
  assume that you are running commands under the repository llama.cpp.

  Command: Run Qwen3 Embedding (Single Query)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ./build/bin/llama-embedding -m model.gguf -p "<your context here>" --pooling last --verbose-prompt
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Command: Launch Server
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ./build/bin/llama-server -m model.gguf --embedding --pooling last -ub 8192 --verbose-prompt
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


  ================================================================================
  5. EVALUATION BENCHMARKS
  ================================================================================

  --------------------------------------------------------------------------------
  5.1 MTEB (Multilingual)
  --------------------------------------------------------------------------------
  Note: For compared models, the scores are retrieved from MTEB online leaderboard 
        on May 24th, 2025.

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Model                           â”‚ Size â”‚ Mean(Task)â”‚ Mean(Type)â”‚ Bitxt   â”‚ Class. â”‚ Clust. â”‚ Inst.Retr â”‚ Multi.Cls. â”‚ Pair.Cls. â”‚ Rerank â”‚ Retri. â”‚ STS   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ NV-Embed-v2                     â”‚ 7B   â”‚ 56.29     â”‚ 49.58     â”‚ 57.84   â”‚ 57.29  â”‚ 40.80  â”‚ 1.04      â”‚ 18.63      â”‚ 78.94     â”‚ 63.82  â”‚ 56.72  â”‚ 71.10 â”‚
  â”‚ GritLM-7B                       â”‚ 7B   â”‚ 60.92     â”‚ 53.74     â”‚ 70.53   â”‚ 61.83  â”‚ 49.75  â”‚ 3.45      â”‚ 22.77      â”‚ 79.94     â”‚ 63.78  â”‚ 58.31  â”‚ 73.33 â”‚
  â”‚ BGE-M3                          â”‚ 0.6B â”‚ 59.56     â”‚ 52.18     â”‚ 79.11   â”‚ 60.35  â”‚ 40.88  â”‚ -3.11     â”‚ 20.10      â”‚ 80.76     â”‚ 62.79  â”‚ 54.60  â”‚ 74.12 â”‚
  â”‚ multilingual-e5-large-instruct  â”‚ 0.6B â”‚ 63.22     â”‚ 55.08     â”‚ 80.13   â”‚ 64.94  â”‚ 50.75  â”‚ -0.40     â”‚ 22.91      â”‚ 80.86     â”‚ 62.61  â”‚ 57.12  â”‚ 76.81 â”‚
  â”‚ gte-Qwen2-1.5B-instruct         â”‚ 1.5B â”‚ 59.45     â”‚ 52.69     â”‚ 62.51   â”‚ 58.32  â”‚ 52.05  â”‚ 0.74      â”‚ 24.02      â”‚ 81.58     â”‚ 62.58  â”‚ 60.78  â”‚ 71.61 â”‚
  â”‚ gte-Qwen2-7B-instruct           â”‚ 7B   â”‚ 62.51     â”‚ 55.93     â”‚ 73.92   â”‚ 61.55  â”‚ 52.77  â”‚ 4.94      â”‚ 25.48      â”‚ 85.13     â”‚ 65.55  â”‚ 60.08  â”‚ 73.98 â”‚
  â”‚ text-embedding-3-large          â”‚ -    â”‚ 58.93     â”‚ 51.41     â”‚ 62.17   â”‚ 60.27  â”‚ 46.89  â”‚ -2.68     â”‚ 22.03      â”‚ 79.17     â”‚ 63.89  â”‚ 59.27  â”‚ 71.68 â”‚
  â”‚ Cohere-embed-multilingual-v3.0  â”‚ -    â”‚ 61.12     â”‚ 53.23     â”‚ 70.50   â”‚ 62.95  â”‚ 46.89  â”‚ -1.89     â”‚ 22.74      â”‚ 79.88     â”‚ 64.07  â”‚ 59.16  â”‚ 74.80 â”‚
  â”‚ gemini-embedding-exp-03-07      â”‚ -    â”‚ 68.37     â”‚ 59.59     â”‚ 79.28   â”‚ 71.82  â”‚ 54.59  â”‚ 5.18      â”‚ 29.16      â”‚ 83.63     â”‚ 65.58  â”‚ 67.71  â”‚ 79.40 â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Qwen3-Embedding-0.6B            â”‚ 0.6B â”‚ 64.33     â”‚ 56.00     â”‚ 72.22   â”‚ 66.83  â”‚ 52.33  â”‚ 5.09      â”‚ 24.59      â”‚ 80.83     â”‚ 61.41  â”‚ 64.64  â”‚ 76.17 â”‚
  â”‚ Qwen3-Embedding-4B              â”‚ 4B   â”‚ 69.45     â”‚ 60.86     â”‚ 79.36   â”‚ 72.33  â”‚ 57.15  â”‚ 11.56     â”‚ 26.77      â”‚ 85.05     â”‚ 65.08  â”‚ 69.60  â”‚ 80.86 â”‚
  â”‚ Qwen3-Embedding-8B              â”‚ 8B   â”‚ 70.58     â”‚ 61.69     â”‚ 80.89   â”‚ 74.00  â”‚ 57.65  â”‚ 10.06     â”‚ 28.66      â”‚ 86.40     â”‚ 65.63  â”‚ 70.88  â”‚ 81.08 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

  Legend:
    â€¢ Bitxt    = Bitext Mining
    â€¢ Class.   = Classification
    â€¢ Clust.   = Clustering
    â€¢ Inst.Retr = Instruction Retrieval
    â€¢ Multi.Cls = Multilabel Classification
    â€¢ Pair.Cls  = Pair Classification
    â€¢ Rerank   = Reranking
    â€¢ Retri.   = Retrieval
    â€¢ STS      = Semantic Textual Similarity

  --------------------------------------------------------------------------------
  5.2 MTEB (English v2)
  --------------------------------------------------------------------------------
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Model                           â”‚ Param. â”‚ Mean(Task)â”‚ Mean(Type)â”‚ Class. â”‚ Clust. â”‚ Pair Cls. â”‚ Rerank. â”‚ Retri. â”‚ STS   â”‚ Summ. â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ multilingual-e5-large-instruct  â”‚ 0.6B   â”‚ 65.53     â”‚ 61.21     â”‚ 75.54  â”‚ 49.89  â”‚ 86.24     â”‚ 48.74   â”‚ 53.47  â”‚ 84.72 â”‚ 29.89 â”‚
  â”‚ NV-Embed-v2                     â”‚ 7.8B   â”‚ 69.81     â”‚ 65.00     â”‚ 87.19  â”‚ 47.66  â”‚ 88.69     â”‚ 49.61   â”‚ 62.84  â”‚ 83.82 â”‚ 35.21 â”‚
  â”‚ GritLM-7B                       â”‚ 7.2B   â”‚ 67.07     â”‚ 63.22     â”‚ 81.25  â”‚ 50.82  â”‚ 87.29     â”‚ 49.59   â”‚ 54.95  â”‚ 83.03 â”‚ 35.65 â”‚
  â”‚ gte-Qwen2-1.5B-instruct         â”‚ 1.5B   â”‚ 67.20     â”‚ 63.26     â”‚ 85.84  â”‚ 53.54  â”‚ 87.52     â”‚ 49.25   â”‚ 50.25  â”‚ 82.51 â”‚ 33.94 â”‚
  â”‚ stella_en_1.5B_v5               â”‚ 1.5B   â”‚ 69.43     â”‚ 65.32     â”‚ 89.38  â”‚ 57.06  â”‚ 88.02     â”‚ 50.19   â”‚ 52.42  â”‚ 83.27 â”‚ 36.91 â”‚
  â”‚ gte-Qwen2-7B-instruct           â”‚ 7.6B   â”‚ 70.72     â”‚ 65.77     â”‚ 88.52  â”‚ 58.97  â”‚ 85.90     â”‚ 50.47   â”‚ 58.09  â”‚ 82.69 â”‚ 35.74 â”‚
  â”‚ gemini-embedding-exp-03-07      â”‚ -      â”‚ 73.30     â”‚ 67.67     â”‚ 90.05  â”‚ 59.39  â”‚ 87.70     â”‚ 48.59   â”‚ 64.35  â”‚ 85.29 â”‚ 38.28 â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Qwen3-Embedding-0.6B            â”‚ 0.6B   â”‚ 70.70     â”‚ 64.88     â”‚ 85.76  â”‚ 54.05  â”‚ 84.37     â”‚ 48.18   â”‚ 61.83  â”‚ 86.57 â”‚ 33.43 â”‚
  â”‚ Qwen3-Embedding-4B              â”‚ 4B     â”‚ 74.60     â”‚ 68.10     â”‚ 89.84  â”‚ 57.51  â”‚ 87.01     â”‚ 50.76   â”‚ 68.46  â”‚ 88.72 â”‚ 34.39 â”‚
  â”‚ Qwen3-Embedding-8B              â”‚ 8B     â”‚ 75.22     â”‚ 68.71     â”‚ 90.43  â”‚ 58.57  â”‚ 87.52     â”‚ 51.56   â”‚ 69.44  â”‚ 88.58 â”‚ 34.83 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

  Legend:
    â€¢ Class.    = Classification
    â€¢ Clust.    = Clustering
    â€¢ Pair Cls. = Pair Classification
    â€¢ Rerank.   = Reranking
    â€¢ Retri.    = Retrieval
    â€¢ STS       = Semantic Textual Similarity
    â€¢ Summ.     = Summarization

  --------------------------------------------------------------------------------
  5.3 C-MTEB (MTEB Chinese)
  --------------------------------------------------------------------------------
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Model                           â”‚ Param. â”‚ Mean(Task)â”‚ Mean(Type)â”‚ Class. â”‚ Clust. â”‚ Pair Cls. â”‚ Rerank. â”‚ Retr.  â”‚ STS   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ multilingual-e5-large-instruct  â”‚ 0.6B   â”‚ 58.08     â”‚ 58.24     â”‚ 69.80  â”‚ 48.23  â”‚ 64.52     â”‚ 57.45   â”‚ 63.65  â”‚ 45.81 â”‚
  â”‚ bge-multilingual-gemma2         â”‚ 9B     â”‚ 67.64     â”‚ 68.52     â”‚ 75.31  â”‚ 59.30  â”‚ 86.67     â”‚ 68.28   â”‚ 73.73  â”‚ 55.19 â”‚
  â”‚ gte-Qwen2-1.5B-instruct         â”‚ 1.5B   â”‚ 67.12     â”‚ 67.79     â”‚ 72.53  â”‚ 54.61  â”‚ 79.50     â”‚ 68.21   â”‚ 71.86  â”‚ 60.05 â”‚
  â”‚ gte-Qwen2-7B-instruct           â”‚ 7.6B   â”‚ 71.62     â”‚ 72.19     â”‚ 75.77  â”‚ 66.06  â”‚ 81.16     â”‚ 69.24   â”‚ 75.70  â”‚ 65.20 â”‚
  â”‚ ritrieve_zh_v1                  â”‚ 0.3B   â”‚ 72.71     â”‚ 73.85     â”‚ 76.88  â”‚ 66.50  â”‚ 85.98     â”‚ 72.86   â”‚ 76.97  â”‚ 63.92 â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Qwen3-Embedding-0.6B            â”‚ 0.6B   â”‚ 66.33     â”‚ 67.45     â”‚ 71.40  â”‚ 68.74  â”‚ 76.42     â”‚ 62.58   â”‚ 71.03  â”‚ 54.52 â”‚
  â”‚ Qwen3-Embedding-4B              â”‚ 4B     â”‚ 72.27     â”‚ 73.51     â”‚ 75.46  â”‚ 77.89  â”‚ 83.34     â”‚ 66.05   â”‚ 77.03  â”‚ 61.26 â”‚
  â”‚ Qwen3-Embedding-8B              â”‚ 8B     â”‚ 73.84     â”‚ 75.00     â”‚ 76.97  â”‚ 80.08  â”‚ 84.23     â”‚ 66.99   â”‚ 78.21  â”‚ 63.53 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

  Legend:
    â€¢ Class.    = Classification
    â€¢ Clust.    = Clustering
    â€¢ Pair Cls. = Pair Classification
    â€¢ Rerank.   = Reranking
    â€¢ Retr.     = Retrieval
    â€¢ STS       = Semantic Textual Similarity


  ================================================================================
                                END OF DOCUMENTATION
  ================================================================================